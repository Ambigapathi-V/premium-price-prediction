{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ambig\\\\jupiter_notebook\\\\Projects\\\\premium-price-prediction\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ambig\\\\jupiter_notebook\\\\Projects\\\\premium-price-prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import  dataclass\n",
    "from typing import List,Dict\n",
    "from pathlib import Path\n",
    "@dataclass\n",
    "class FeatureEngineeringConfig:\n",
    "    root_dir: Path\n",
    "    input_path: Path\n",
    "    output_path: Path\n",
    "    model_path: Path\n",
    "    test_filepath : Path\n",
    "    train_filepath : Path\n",
    "    target_column : str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Premium_Price_Prediction.constants import *\n",
    "from src.Premium_Price_Prediction.utils.common import read_yaml , create_directories , load_df , save_object , save_object\n",
    "from src.Premium_Price_Prediction import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath=CONFIG_FILE_PATH, \n",
    "                 params_filepath=PARAMS_FILE_PATH, \n",
    "                 schema_filepath=SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_feature_engineering_config(self) -> FeatureEngineeringConfig:\n",
    "        config = self.config.feature_engineering\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        \n",
    "        feature_engineering_config = FeatureEngineeringConfig(\n",
    "            root_dir=Path(config[\"root_dir\"]),\n",
    "            input_path=Path(config[\"input_path\"]),\n",
    "            output_path=Path(config[\"output_path\"]),\n",
    "            model_path=Path(config[\"model_path\"]),\n",
    "            test_filepath=Path(config[\"test_filepath\"]),\n",
    "            train_filepath=Path(config[\"train_filepath\"]),\n",
    "            target_column = config[\"target_column\"]\n",
    "        )\n",
    "        return feature_engineering_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "class FeatureEngineering:\n",
    "    def __init__(self, config: FeatureEngineeringConfig):\n",
    "        self.config = config\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.preprocessor = None\n",
    "\n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load training and testing datasets from specified file paths.\"\"\"\n",
    "        self.train_df = pd.read_csv(self.config.train_filepath)\n",
    "        self.test_df = pd.read_csv(self.config.test_filepath)\n",
    "        logger.info(f\"Data loaded from {self.config.train_filepath} and {self.config.test_filepath}\")\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def preprocess_medical_history(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process the 'medical_history' column and compute risk scores.\"\"\"\n",
    "        risk_scores = {\n",
    "            \"diabetes\": 6,\n",
    "            \"heart disease\": 8,\n",
    "            \"high blood pressure\": 6,\n",
    "            \"thyroid\": 5,\n",
    "            \"no disease\": 0,\n",
    "            \"none\": 0\n",
    "        }\n",
    "\n",
    "        # Split and map risk scores\n",
    "        df[['disease1', 'disease2']] = (\n",
    "            df['medical_history'].str.split(\" & \", expand=True).apply(lambda x: x.str.lower())\n",
    "        ).fillna('none')\n",
    "\n",
    "        df['total_risk_score'] = df[['disease1', 'disease2']].apply(\n",
    "            lambda x: risk_scores.get(x['disease1'], 0) + risk_scores.get(x['disease2'], 0), axis=1\n",
    "        )\n",
    "\n",
    "        # Normalize risk scores\n",
    "        max_score, min_score = df['total_risk_score'].max(), df['total_risk_score'].min()\n",
    "        df['normalized_risk_score'] = (df['total_risk_score'] - min_score) / (max_score - min_score)\n",
    "        logger.info(\"Risk scores calculated and normalized\")\n",
    "        return df\n",
    "\n",
    "    def build_preprocessor(self):\n",
    "        \"\"\"Construct the preprocessing pipeline for numerical and categorical features.\"\"\"\n",
    "        # Identify numerical and categorical columns\n",
    "        numerical_columns = self.train_df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "        categorical_columns = self.train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "        # Define preprocessing transformers\n",
    "        numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "        categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', drop=\"first\"))])\n",
    "\n",
    "        # Combine transformers into a preprocessor\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_columns),\n",
    "                ('cat', categorical_transformer, categorical_columns)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "        logger.info(\"Preprocessing pipeline built\")\n",
    "\n",
    "    def fit_transform_data(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Fit the preprocessor and transform the datasets.\"\"\"\n",
    "        X_train_transformed = self.preprocessor.fit_transform(self.train_df)\n",
    "        X_test_transformed = self.preprocessor.transform(self.test_df)\n",
    "        logger.info(\"Transformations applied to the dataset\")\n",
    "        return X_train_transformed, X_test_transformed\n",
    "\n",
    "    def save_object(self, file_path: Path, obj):\n",
    "        \"\"\"Save an object to a specified file path using joblib.\"\"\"\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        try:\n",
    "            joblib.dump(obj, file_path)\n",
    "            logger.info(f\"Object saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save object to {file_path}: {str(e)}\")\n",
    "            raise  # Raise the exception after logging\n",
    "\n",
    "    def feature_engineering(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Execute the feature engineering pipeline: load data, process, transform, and save.\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess medical history data\n",
    "            self.train_df, self.test_df = self.load_data()\n",
    "\n",
    "            # Check if 'annual_premium_amount' exists\n",
    "            if 'annual_premium_amount' not in self.train_df.columns:\n",
    "                logger.error(\"Column 'annual_premium_amount' not found in training data.\")\n",
    "                raise KeyError(\"Column 'annual_premium_amount' not found in training data.\")\n",
    "\n",
    "            # Preprocess medical history\n",
    "            self.train_df = self.preprocess_medical_history(self.train_df)\n",
    "            self.test_df = self.preprocess_medical_history(self.test_df)\n",
    "\n",
    "            # Build the preprocessor\n",
    "            self.build_preprocessor()\n",
    "\n",
    "            # Fit and transform data\n",
    "            train_data, test_data = self.fit_transform_data()\n",
    "\n",
    "            # Save the preprocessor and transformed data\n",
    "            self.save_object(self.config.output_path / 'preprocessor.joblib', self.preprocessor)\n",
    "\n",
    "            # Save transformed data\n",
    "            pd.DataFrame(train_data).to_csv(self.config.output_path / 'train_data.csv', index=False)\n",
    "            pd.DataFrame(test_data).to_csv(self.config.output_path / 'test_data.csv', index=False)\n",
    "\n",
    "            logger.info(\"Transformed data saved successfully.\")\n",
    "            return train_data, test_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during feature engineering: {str(e)}\")\n",
    "            raise  # Maintain the original error handling for external catch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-04 14:06:00,472: INFO: common: 30] YAML file : config\\config.yaml loaded successfully\n",
      "[2024-11-04 14:06:00,475: INFO: common: 30] YAML file : params.yaml loaded successfully\n",
      "[2024-11-04 14:06:00,478: INFO: common: 30] YAML file : schema.yaml loaded successfully\n",
      "[2024-11-04 14:06:00,480: INFO: common: 50] Directory artifacts created successfully.\n",
      "[2024-11-04 14:06:00,482: INFO: common: 50] Directory artifacts/feature_engineering created successfully.\n",
      "[2024-11-04 14:06:00,557: INFO: 2080276664: 24] Data loaded from artifacts\\data_preprocessing\\train.csv and artifacts\\data_preprocessing\\test.csv\n",
      "[2024-11-04 14:06:01,019: INFO: 2080276664: 50] Risk scores calculated and normalized\n",
      "[2024-11-04 14:06:01,103: INFO: 2080276664: 50] Risk scores calculated and normalized\n",
      "[2024-11-04 14:06:01,120: INFO: 2080276664: 71] Preprocessing pipeline built\n",
      "[2024-11-04 14:06:01,289: INFO: 2080276664: 77] Transformations applied to the dataset\n",
      "[2024-11-04 14:06:01,289: INFO: 2080276664: 85] Object saved to artifacts\\feature_engineering\\preprocessor.joblib\n",
      "[2024-11-04 14:06:02,490: INFO: 2080276664: 118] Transformed data saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.22652497, -1.14451085, -0.93676115, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [-0.83342392, -0.47448297, -0.83075245, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [ 0.09403136,  0.1955449 ,  2.40251319, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        ...,\n",
       "        [-0.56843669, -0.47448297,  2.19049578, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [-0.17095586, -1.14451085, -0.51272632, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [-0.6346835 ,  0.1955449 , -0.14169583, ...,  0.        ,\n",
       "          1.        ,  0.        ]]),\n",
       " array([[-1.03216433, -1.14451085, -0.8837568 , ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [-0.56843669, -1.14451085, -0.67173938, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [ 1.28647385,  0.86557278, -0.40671761, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        ...,\n",
       "        [-1.09841114, -0.47448297,  0.97139562, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [ 2.34642274, -0.47448297, -0.93676115, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [ 0.16027816,  1.53560065, -0.56573067, ...,  0.        ,\n",
       "          0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "feature_engineering_config = config.get_feature_engineering_config()  # Corrected variable name\n",
    "feature_engineering = FeatureEngineering(feature_engineering_config)\n",
    "feature_engineering.feature_engineering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
